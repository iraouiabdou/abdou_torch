# abdou_torch â­â­â­
This is a minimalistic deep learning framework implemented purely in Python and NumPy.

Inspired by the mechanics of modern libraries like PyTorch and Andrej Karpathy's micrograd, this repository aims to replicate the core components required for training neural networks, specifically focusing on automatic differentiation (autograd).

### ğŸ“Core Design Principles

**1. The Tensor Class ğŸ§Š**

The core data structure is the Tensor. It is a wrapper around a NumPy array that is responsible for:

- Holding the numerical data (.data).

- Storing the derivative (.grad).

- Tracking the computational history via its parents (._prev) and the operation that created it (._op).

**2. Backpropagation via the Chain Rule ğŸ”„**

The backward() method is the heart of the project. It uses a topological sort to process the computation graph in reverse order, ensuring that gradients are correctly accumulated and propagated through every node (tensor) based on the chain rule.

**3. Operator overloading â•**

Essential mathematical operations (+, *, **, @, /) and functional layers (tanh, relu, sum) are implemented using Python's operator overloading (__add__, __mul__, etc.).

**4. Neural Network Modules ğŸ§ **

The framework includes simple object-oriented components:

- Module: A base class for parameter management.

- Linear: Implements the standard affine transformation ($W \cdot x + b$).

- MLP: Stacks linear layers and non-linearities to create a full Multi-Layer Perceptron.

**5. Custom WAdam Optimizer ğŸ¯**

This project features a custom implementation of the WAdam optimizer, which combines the adaptive learning rates and momentum of Adam with Decoupled Weight Decay for regularization, providing a robust update rule for parameter optimization.


### ğŸš€ğŸš€ğŸš€Demo: Training a simple MLP

I made a demo for the make_blobs classification and the MNIST dataset.
